  <div class="row section_heading">
    <h2>Projects</h2>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Are we asking the right questions in MovieQA?</h3>
<!--         <p class="pubd">
          <span class="authors"><i><b>Tejas Khot*</b>, Shubham Agrawal*, Shubham Tulsiani, Christoph Mertz, Simon Lucey, Martial Hebert </i></span>
          <br><span class="conf"><a><font color='red'>Under Review</font></a></span>
        </p>
 -->
        <div class="col-xs-12">
          <!-- <a href="https://arxiv.org/abs/1808.00671">
            <img class="thumb" src="/static/img/thumb_unmvs.png" width="100%">
          </a> -->
          <a href="/files/BJ_MovieQA_paper_2019_website.pdf">
            <img class="thumb" src="/files/MovieQA.png" width="50%" align="middle">
          </a>
        </div>

        <p class="content">Joint vision and language tasks like visual question an-swering are fascinating because they explore high levelunderstanding, but at the same time, can be more proneto language biases. In this paper we explore the bi-ases in the MovieQA dataset and propose a strikingly sim-ple model which can exploit them. We found that usingright word embedding is of utmost importance. By usingan appropriately-trained word embedding, about half theQuestion-Answers (QAs) can be answered by looking at thequestions and answers alone, completely ignoring narrativecontext from video clips, subtitles, and movie scripts. Com-pared to the best published models on the leaderboard, oursimple question+answer only model improves accuracy by 5% for video + subtitle category, 5% for subtitle, 15% forDVS and 6% higher for scripts. We further propose a solu-tion to mitigate these language biases by creating a subsetof hard questions that require additional contextual cues to answer.<br>
        </p>

        <p>
          <!-- <a target="_blank" href="https://arxiv.org/abs/1808.00671" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="https://www.cs.cmu.edu/~wyuan1/pcn/" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=bzJrPQilPxg" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a> -->
          <a target="_blank" href="/files/BJ_MovieQA_paper_2019_website.pdf" class="white-text"><button type="button" class="btn btn-primary ribbon">PDF</button></a>
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Skeleton based Zero Shot Action Recognition using a Learnable Distance Metric in Joint Pose Language Semantic Space</h3>
<!--         <p class="pubd">
          <span class="authors"><i>Wentao Yuan, <b>Tejas Khot</b> , David Held, Christoph Mertz, Martial Hebert </i></span>
          <br><span class="conf"><a>3DV 2018 - <font color='red'>Oral, Honorable Mention for Best Paper Award</font></a></span>
        </p>
 -->
        <div class="col-xs-12">
          <a href="/files/wacv2019_paper_BJ_website.pdf">
            <img class="thumb" src="/files/Zero_shot_learning.png" width="50%%" align="middle">
          </a>
        </div>

        <p class="content">How does one represent an action ? How does one describe an action that we have never seen before ? Such questions are addressed by the Zero Shot Learning paradigm, where a model is trained on only a subset of classes and is evaluated on its ability to correctly classify an example from a class it has never seen before. In this work, we present a Pose based Zero Shot Action Recogni- tion System and demonstrate its performance on the NTU- RGB-D dataset in the Cross View setting. We explore the significance of using pose information as the ideal repre- sentation for understanding actions in videos. In particular, we believe that learning human dynamics in a context free setting (i.e. just focusing on the action performer and not the context - the surroundings) allows the model to capture more information within the visual domain so that this infor- mation can be transferred easily to visually similar classes unseen during training.<br>
        </p>

        <p>
          <a target="_blank" href="/files/wacv2019_paper_BJ_website.pdf" class="white-text"><button type="button" class="btn btn-primary ribbon">PDF</button></a>

 <!--          <a target="_blank" href="https://arxiv.org/abs/1808.00671" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
          <a target="_blank" href="https://www.cs.cmu.edu/~wyuan1/pcn/" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=bzJrPQilPxg" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a>
 -->        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>

    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Learning Sampling Policies for Domain Adaptation</h3>
<!--         <p class="pubd">
          <span class="authors"><i>Yash Goyal<sup>*</sup>, <b>Tejas Khot</b><sup>*</sup>, Douglas Summers-Stay, Dhruv Batra, Devi Parikh</i></span>
          <br><span class="conf"><a>CVPR 2017, IJCV 2018</a></span>
        </p>
 -->
        <!-- <p>
          <a href="https://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
        </p> -->

        <div class="col-xs-12">
          <a href="/files/1805.07641_arvix_learning_sampling.pdf">
            <img class="thumb" src="/files/Domain_adaptation.png" width="50%">
          </a>
        </div>

        <p class="content">We address the problem of semi-supervised domain adapta- tion of classification algorithms through deep Q-learning. The core idea is to consider the predictions of a source domain network on target domain data as noisy labels, and learn a policy to sample from this data so as to maximize classification accuracy on a small annotated reward partition of the target domain. Our experiments show that learned sampling poli- cies construct labeled sets that improve accuracies of visual classifiers over baselines.<br>
        </p>

        <p>
          <a target="_blank" href="https://arxiv.org/abs/1805.07641" class="white-text"><button type="button" class="btn btn-primary ribbon">Arxiv</button></a>
<!--           <a target="_blank" href="http://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
          <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE" class="white-text"><button type="button" class="btn btn-primary ribbon">YouTube Demo</button></a> -->
        </p>
        <!-- <a target="_blank" href="https://arxiv.org/abs/1612.00837">[arxiv] </a>&nbsp -->
        <!-- <a target="_blank" href="http://www.visualqa.org">[Project Page] </a>&nbsp -->
        <!-- <a target="_blank" href="https://www.youtube.com/watch?v=nMr_sSAMpkE">[YouTube Demo] </a> -->
      </div>
    </div>


    <hr>
    <div class="row">
      <div class="col-md-12">
        <h3 class="content_heading">Adversarial image generation using GAN</h3>
<!--         <p class="pubd">
          <span class="authors"><i>Yash Goyal<sup>*</sup>, <b>Tejas Khot</b><sup>*</sup>, Douglas Summers-Stay, Dhruv Batra, Devi Parikh</i></span>
          <br><span class="conf"><a>CVPR 2017, IJCV 2018</a></span>
        </p>
 -->
        <!-- <p>
          <a href="https://visualqa.org" class="white-text"><button type="button" class="btn btn-primary ribbon">Project Webpage</button></a>
        </p> -->

        <div class="col-xs-12">
          <a href="/files/CV_project_slides.pdf">
            <img class="thumb" src="/files/Adversarial_image_generation.png" width="50%">
          </a>
        </div>

        <p class="content">Implemented and trained a generative adversarial network (GAN) for generating adversarial images for CIFAR 10 dataset for black box attacks<br>
        </p>

        <p>
          <a target="_blank" href="/files/CV_project_slides.pdf" class="white-text"><button type="button" class="btn btn-primary ribbon">PDF</button></a>
        </p>
      </div>
    </div>



  </div>
